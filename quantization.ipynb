{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quantization",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4HQauY6RivH",
        "colab_type": "text"
      },
      "source": [
        "**Post-training quantization and quantized inference**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USkKyQVHzvbr",
        "colab_type": "text"
      },
      "source": [
        "Post-training quantization is a conversion technique that can **reduce model size** while also **improving CPU and hardware accelerator latency**, with little **degradation** in model **accuracy.** You can perform these techniques using an already-trained **float** TensorFlow model when you convert it to TensorFlow Lite format. Once a model is **fully quantized**, you can deploy it to platforms like **coral tpu** or run it using **nnapi** **delegates**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK8kRTkTSIt7",
        "colab_type": "text"
      },
      "source": [
        " Install latest **tf-nightly** for quantization and tflite inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWPepcH3AmRp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tf-nightly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoSc7RJKA0_Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS4afMkHWXkT",
        "colab_type": "text"
      },
      "source": [
        "The following diagram shows the various methods for **post-training quantization** available in tensorflow-lite.\n",
        "\n",
        "![TfliteQuantizatiom](https://www.tensorflow.org/lite/performance/images/optimization.jpg)\n",
        "\n",
        "Here we will use **full integer quantization**(INT8) with a representative dataset. Use the **trainig data-set** as the representative dataset for this purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHKPHxqmAxmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip /content/drive/My\\ Drive/portrait_mix/EG1800.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51CWZpJ7G8yT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import libraries\n",
        "import tensorflow as tf\n",
        "import cv2, sys, time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import timeit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5lGb-v5Ltfx",
        "colab_type": "text"
      },
      "source": [
        "To convert a float32 model to quantized int8 format, there are two approaches viz. **quantizaton aware training** and **post training quantization**. Here we use the latter approach of quantization, to convert our model to **INT8** format. We need to quantize both the weights and the corresponding layers to make use of accelerators like **Coral TPU or NNAPI**. We will use the training dataset as a **representative dataset** to measure the dynamic range of activations and inputs.\n",
        "\n",
        "**Note:** TF 2.0/Keras currently does not support quantization aware training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoaAcChbAWE0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Configure dataset directories\n",
        "IMGDS='/content/EG1800/imgs'\n",
        "MSKDS='/content/EG1800/msks'\n",
        "\n",
        "# Use image data generator to load the dataset\n",
        "image_generator = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "seed=7\n",
        "\n",
        "train_image_generator = image_generator.flow_from_directory(\n",
        "    IMGDS,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    target_size=(224, 224),\n",
        "    color_mode=\"rgb\",\n",
        "    class_mode=None,\n",
        "    seed=seed)\n",
        "\n",
        "train_mask_generator = image_generator.flow_from_directory(\n",
        "    MSKDS,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    target_size=(224, 224),\n",
        "    color_mode=\"grayscale\",\n",
        "    class_mode=None,\n",
        "    seed=seed)\n",
        "\n",
        "# Normalize the input image\n",
        "def normalize(imgOri, scale=1, mean=[103.94, 116.78, 123.68], val=[0.017, 0.017, 0.017]):\n",
        "    img = np.array(imgOri.copy(), np.float32)/scale\n",
        "    return (img - mean) * val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtpsLHTGOw_O",
        "colab_type": "text"
      },
      "source": [
        "Use the keras **data generator** to load the dataset and perform the data augmentaion(like training) to generate the inputs for the quantization process. Configure the tflite converter for **full integer quantization** and convert the model to **INT8 tflite** format. Now the saved model will have **INT8 Range: -128 to 127**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bsHaxQcAdPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the dataset with data augmentations\n",
        "def representative_dataset_gen():\n",
        "  for _ in range(100):\n",
        "    imgs=train_image_generator.next()\n",
        "    msks=train_mask_generator.next()\n",
        "    input=np.zeros(shape=(8,224,224,4), dtype=np.float32)\n",
        "    for i in range(len(imgs)):\n",
        "      img=normalize(cv2.cvtColor(imgs[i],cv2.COLOR_RGB2BGR))\n",
        "      msk=cv2.normalize(cv2.blur(msks[i], (5,5)), 0, 1, cv2.NORM_MINMAX) # partial augmentation\n",
        "      input=np.float32(np.dstack([img,msk]).reshape(1,224,224,4))\n",
        "\n",
        "      yield [input]\n",
        "\n",
        "# Convert the model to INT8 format\n",
        "converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file('portrait_video_flattened.h5')\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_dataset_gen\n",
        "\n",
        "# Save the quantized model\n",
        "tflite_quant_model = converter.convert()\n",
        "open(\"/content/portrait_video_quant.tflite\", \"wb\").write(tflite_quant_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtLiLK5gP3fO",
        "colab_type": "text"
      },
      "source": [
        "Test the new quantized model on a new **video file**, with the help of opencv. Make sure to convert the float input to **INT8 range** before inference and INT8 output to **float32** after quantized inference. Save the outputs using opencv video writer and **compare** the output with the original results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxzpZ_a7AhtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalize the input image\n",
        "def normalize(imgOri, scale=1, mean=[103.94, 116.78, 123.68], val=[0.017, 0.017, 0.017]):\n",
        "    img = np.array(imgOri.copy(), np.float32)/scale\n",
        "    norm=(img - mean) * val\n",
        "    out=cv2.normalize(src=norm, dst=None, alpha=-128, beta=127, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8S)\n",
        "    return out\n",
        " \n",
        "# Alpha blend frame with background\n",
        "def blend(frame, alpha):\n",
        "        background = np.zeros(frame.shape) + [255, 255, 255]\n",
        "        alphargb = cv2.cvtColor(alpha, cv2.COLOR_GRAY2BGR)\n",
        "        result = np.uint8(frame * alphargb + background * (1-alphargb))\n",
        "        return frame, alphargb*255, result\n",
        "\n",
        "# Initialize tflite-interpreter\n",
        "interpreter = tf.lite.Interpreter(model_path=\"portrait_video_fquant.tflite\") # Use 'tf.lite' on recent tf versions\n",
        "interpreter.allocate_tensors()\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "input_shape = input_details[0]['shape'][1:3]\n",
        "\n",
        "\n",
        "# Initialize video capturer\n",
        "videofile = 'portrait_lady.mp4'\n",
        "cap = cv2.VideoCapture(videofile)  \n",
        "size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))  \n",
        "videoWriter = cv2.VideoWriter('result.mp4', cv2.VideoWriter_fourcc(*'MJPG'), 20, size)  \n",
        " \n",
        "# Initialize frame counter \n",
        "cnt = 1\n",
        "\n",
        "while True:\n",
        "       \n",
        "    # Read the BGR frames \n",
        "    success, frame = cap.read()\n",
        "    if not(success):\n",
        "      break\n",
        "    image=Image.fromarray(frame)\n",
        "    \n",
        "    \n",
        "    # Resize the image\n",
        "    image= image.resize(input_shape, Image.ANTIALIAS)\n",
        "    image=np.asarray(image)\n",
        "\n",
        "    \n",
        "    # Normalize the input\n",
        "    image = normalize(image)\n",
        "  \n",
        "    # Choose prior mask\n",
        "    if cnt == 1:\n",
        "        prior = np.full((224, 224, 1),-128,dtype=np.int8) # first frame\n",
        "    else:\n",
        "        prior = pred_video\n",
        "    \n",
        "    # Add prior as fourth channel\n",
        "    image=np.dstack([image,prior])\n",
        "    prepimg = image[np.newaxis, :, :, :]\n",
        "    \n",
        "\n",
        "    # Invoke interpreter for inference\n",
        "    interpreter.set_tensor(input_details[0]['index'], np.array(prepimg, dtype=np.int8))\n",
        "    interpreter.invoke()\n",
        "    outputs = interpreter.get_tensor(output_details[0]['index'])\n",
        "    outputs = outputs.reshape(224,224,1)\n",
        "  \n",
        "    # Save output to feed subsequent inputs\n",
        "    pred_video = outputs\n",
        "\n",
        "    # Process the output and perform alpha blending\n",
        "    outputs = outputs.astype('float32')  \n",
        "    outputs = cv2.resize(outputs, size)\n",
        "    outputs= cv2.normalize(src=outputs, dst=None, alpha=0.0, beta=1.0, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "    _,_,outputs=blend(frame, outputs)\n",
        "        \n",
        "    # Write the output frame\n",
        "    videoWriter.write(outputs)\n",
        "    \n",
        "    # Print the frame count\n",
        "    cnt += 1\n",
        "    if cnt % 100 == 0:\n",
        "        print (\"cnt: \", cnt)\n",
        "\n",
        "# When everything done, release the capturer\n",
        "print(\"Conversion successfull !!!\")\n",
        "videoWriter.release()\n",
        "cap.release()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9RZhlE-VKoV",
        "colab_type": "text"
      },
      "source": [
        "Older versions of **tensorflow**(1.x) does not seem to support conversions for quantized versions of **transpose convolutions** using the current approach. Compared to the original float model, the output with quantized model looks **less accurate**. Hopefully, the resut may be slighlty improved with **full data augmentation**  and/or **quantization aware training**. Unfortunately the fully quantized model could not be converted to **coral tpu** format due to unsupported layers and compatability issues; but it may work with the latest **NNAPI** delegate in android."
      ]
    }
  ]
}